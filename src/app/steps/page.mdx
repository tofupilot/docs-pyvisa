export const metadata = {
  title: 'Steps',
  description: '',
}

# Steps

Production tests often consist of multiple successive steps: connection, measurements, calibration, etc. TofuPilot enables precise logging of each step and provides detailed step analytics across all runs. {{ className: 'lead' }}

This allows you to quickly identify your most frequent failing tests and time-consuming steps, ensuring you focus on processes with the most potential for improvement.

## Example

To log test step details, the code defines test functions, runs them sequentially, captures their results, and adds these details to the `steps` table in `create_run`.

```python {{ title: 'create_run_with_steps.py'}}
from tofupilot import TofuPilotClient
from datetime import datetime

client = TofuPilotClient(api_key="Your API key")

# Simple true/false step
def step_connect():
    return True,

# Step with a measurement value and unit
def step_initial_charge_check():
    initial_charge = 80
    return True, initial_charge, "%"

# Step with a measurement and a lower limit
def step_initial_temp_check():
    initial_temp = 72.0
    limit_low = 0.0
    return initial_temp >= limit_low, initial_temp, "°C", limit_low

# Step with a measurement, lower limit, and upper limit
def step_temp_calibration():
    calibrated_temp = 75.0
    limit_low = 70.0
    limit_high = 80.0
    return limit_low <= calibrated_temp <= limit_high, calibrated_temp, "°C", limit_low, limit_high

# Simple step sequencer and logger
def run_all_tests():
    tests = [func for name, func in globals().items() if callable(func) and name.startswith("step_")]
    steps, all_passed = [], True

    for t in tests:
        start = datetime.now()
        passed, value, unit, low, high = (t() + (None,) * 4)[:5]  # unpack with default None
        end = datetime.now()
        steps.append({
            "name": t.__name__,
            "started_at": start,
            "duration": end - start,
            "step_passed": passed,
            "measurement_value": value,
            "measurement_unit": unit,
            "limit_low": low,
            "limit_high": high
        })
        all_passed &= passed

    return all_passed, steps

run_passed, steps = run_all_tests()

client.create_run(
    procedure_id="FVT1",
    unit_under_test={"serial_number": "0001"},
    run_passed=run_passed,
    steps=steps,
)
```

## Run Page

A dedicated Steps section is automatically created on the Run’s page.

<Image src={'/steps-run.png'} alt={'Steps of Run'} />

## Procedure Page

The Procedure page provides a summary of the steps' run count, status, and average duration.

<Image src={'/steps-procedure.png'} alt={'Steps Runs of Procedures'} />

## Test Framework

The provided code example is designed to be framework-agnostic, offering a simple logic suitable for quick implement. For real world deployment, we recommend using your own test sequencer or open-source alternatives like [PyTest](https://docs.pytest.org/en/8.2.x/) or [OpenHTF](https://www.openhtf.com/).

## API Reference

### Mandatory

<Properties>
  <Property name="name" type="string">
    Name of the step.
  </Property>
  <Property name="started_at" type="datetime">
    Timestamp when the step started.
  </Property>
  <Property name="duration" type="float">
    Duration of the step in seconds.
  </Property>
  <Property name="step_passed" type="bool">
    Indicates whether the step passed.
  </Property>
</Properties>

### Optional

<Properties>
  <Property name="measurement_value" type="float or None">
    Value measured during the step, if any.
  </Property>
  <Property name="measurement_unit" type="string or None">
    Unit of the measurement value, if any.
  </Property>
  <Property name="limit_low" type="float or None">
    Lower limit for the measurement, if any.
  </Property>
  <Property name="limit_high" type="float or None">
    Upper limit for the measurement, if any.
  </Property>
</Properties>
